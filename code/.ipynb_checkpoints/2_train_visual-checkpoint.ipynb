{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8aac515-40f0-4fc5-ad7f-49c7a1811409",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Multisensor fusion \n",
    "\n",
    "Author: Chen Lequn.\n",
    "Created on 01 Feb 2023.\n",
    "\n",
    "- Material： Maraging Steel 300\n",
    "- Process: Robotic Llser-directed energy deposition\n",
    "- Experiment number (single bead wall samples): 21-26\n",
    "- Recorded data: position, veolocity, coaxial melt pool images, acoustic data\n",
    "- Defect generated: keyhole pores, cracks, defect-free\n",
    "\n",
    "### Notebook 2: benchmarking on coaxial vision dataset\n",
    "\n",
    "- Performances for various deep learning models (VGG, GoogleNet, ResNet, LeNet, etc.) are evaluated and benchmarked on the coaxial melt pool image dataset.\n",
    "- Best performance will guide subsequent multimodal fusion network development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f5b265-a77f-4440-8cb9-0be328194d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from torch.utils.data import Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit learn\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle, resample, class_weight\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "from vision_models import *\n",
    "from multimodaldataset import MultimodalDataset, LDEDAudioDataset, LDEDVisionDataset\n",
    "from utils import progress_bar\n",
    "\n",
    "## plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "# plt.rcParams[\"font.family\"] = \"serif\"\n",
    "# plt.rcParams[\"font.serif\"] = \"Times New Roman\"\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "Multimodal_dataset_PATH = os.path.join(\"C:\\\\Users\\\\Asus\\\\OneDrive_Chen1470\\\\OneDrive - Nanyang Technological University\\\\Dataset\\\\Multimodal_AM_monitoring\\\\LDED_Acoustic_Visual_Dataset\")\n",
    "CCD_Image_30Hz_path = os.path.join(Multimodal_dataset_PATH, 'Coaxial_CCD_images_30Hz')\n",
    "Audio_segmented_30Hz_PATH = os.path.join(Multimodal_dataset_PATH, 'Audio_signal_all_30Hz')\n",
    "Audio_raw_seg_PATH = os.path.join(Audio_segmented_30Hz_PATH, 'raw')\n",
    "Audio_equalized_seg_PATH = os.path.join(Audio_segmented_30Hz_PATH, 'equalized')\n",
    "Audio_bandpassed_seg_PATH = os.path.join(Audio_segmented_30Hz_PATH, 'bandpassed')\n",
    "Audio_denoised_seg_PATH = os.path.join(Audio_segmented_30Hz_PATH, 'denoised')\n",
    "AUDIO_DIR = Audio_denoised_seg_PATH\n",
    "VISON_DIR = CCD_Image_30Hz_path\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "ANNOTATIONS_FILE = os.path.join(Multimodal_dataset_PATH, \"vision_acoustic_label_v2.csv\")\n",
    "\n",
    "classes = ('Defect-free', 'Cracks', 'Keyhole pores', 'Laser-off', 'Laser-start')\n",
    "SAMPLE_RATE = 44100\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9805f-91a7-4b62-bd59-e80f0f97d40e",
   "metadata": {},
   "source": [
    "### Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363418fa-80c7-4933-b728-4079f921e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT_DIR = \"../\"\n",
    "IMAGE_PATH = os.path.join(PROJECT_ROOT_DIR, \"result_images\", 'train_visual')\n",
    "os.makedirs(IMAGE_PATH, exist_ok=True)\n",
    "\n",
    "## function for automatically save the diagram/graph into the folder \n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGE_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "plt.rcParams[\"axes.linewidth\"] = 2.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db15531f-d797-4136-8748-239a6c0de008",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Loss and Accuracy plot function\n",
    "def loss_acc_plot(historyname, epochs_num, title, interval=20, yloss_limit1=0, yloss_limit2=1.5, yacc_limit1=0.4, yacc_limit2=1):\n",
    "    fig, (ax1,ax2) = plt.subplots(nrows = 2, sharex = True, figsize=(7,8));\n",
    "    # plt.title(title, fontsize = 20, y=1.05)\n",
    "    # Loss plot\n",
    "    ax1.plot(historyname.history['loss'], 'darkorange', label = 'Train Loss', linewidth=2)\n",
    "    ax1.plot(historyname.history['val_loss'], 'navy', label = 'Test Loss', linewidth=2)\n",
    "    ax1.legend(loc =1, fontsize = 16)\n",
    "    ax1.set_xlabel('Epochs', fontsize = 20)\n",
    "    ax1.set_xticks(np.arange(0,epochs_num+1,interval))\n",
    "    ax1.set_ylabel('Crossentropy Loss', fontsize = 20)\n",
    "    ax1.set_ylim(yloss_limit1,yloss_limit2)\n",
    "    ax1.set_title('Loss Curve', fontsize = 20, pad=12)\n",
    "    ax1.xaxis.set_tick_params(labelsize=18)\n",
    "    ax1.yaxis.set_tick_params(labelsize=18)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(historyname.history['accuracy'], 'darkorange', label = 'Train Accuracy', linewidth=2)\n",
    "    ax2.plot(historyname.history['val_accuracy'], 'navy', label = 'Test Accuracy', linewidth=2)\n",
    "    ax2.legend(loc =4, fontsize = 16)\n",
    "    ax2.set_xlabel('Epochs', fontsize = 20)\n",
    "    ax1.set_xticks(np.arange(0,epochs_num+1,interval))\n",
    "    ax2.set_ylabel('Accuracy', fontsize =20)\n",
    "    ax2.set_ylim(yacc_limit1,yacc_limit2)\n",
    "    ax2.set_title('Accuracy Curve', fontsize =20, pad=12)\n",
    "    ax2.xaxis.set_tick_params(labelsize=18)\n",
    "    ax2.yaxis.set_tick_params(labelsize=18)\n",
    "    ax1.grid(zorder=3, linestyle='--',linewidth=0.8, alpha=0.4, color = \"k\") #linestyle='--', color='r'\n",
    "    ax2.grid(zorder=3, linestyle='--',linewidth=0.8, alpha=0.4, color = \"k\") #linestyle='--', color='r'\n",
    "    # fig.suptitle(title, fontsize = 22, y=1.001)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "### Function to print out the model's loss and accuracy score\n",
    "def xtest_loss_acc(modelname, X_test, y_test):\n",
    "    \n",
    "    model_score = modelname.evaluate(X_test, y_test, verbose =2)\n",
    "    model_labels = modelname.metrics_names\n",
    "    \n",
    "    print(f\"cnn {model_labels[0]}: {round(model_score[0] ,5)}\")\n",
    "    print(f\"cnn {model_labels[1]}: {round(model_score[1] ,5)}\")\n",
    "    \n",
    "    \n",
    "### Define function to predict X_test, return y_pred & y_true and print the classification report\n",
    "def class_report(modelname, X_test, y_test, le):\n",
    "    ### predict the X_test\n",
    "    # pred = modelname.predict_classes(X_test) # deprecated\n",
    "    predict_x=modelname.predict(X_test) \n",
    "    pred=np.argmax(predict_x,axis=1)\n",
    "    \n",
    "    # compile predicted results\n",
    "    y_true, y_pred = [], []\n",
    "    classes = le.classes_\n",
    "    \n",
    "    for idx, preds in enumerate(pred):\n",
    "        y_true.append(classes[np.argmax(y_test[idx])])\n",
    "        y_pred.append(classes[preds])\n",
    "    \n",
    "    print(classification_report(y_true, y_pred,digits=4))\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "### Function to plot confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(7,7))\n",
    "    im_ratio = cm.shape[1]/cm.shape[0]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=20, pad=12)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=12)\n",
    "    plt.yticks(tick_marks, classes, fontsize=12)\n",
    "\n",
    "    fmt = '.3f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 fontsize = 16, \n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('Ground Truth', fontsize=20, labelpad =12)\n",
    "    plt.xlabel('Predicted', fontsize=20, labelpad =12)\n",
    "    plt.xticks(fontsize=16,  rotation=45, ha='right')\n",
    "    plt.yticks(fontsize=16)\n",
    "    cbar = plt.colorbar(orientation=\"vertical\", pad=0.1, ticks=[0.1, 0.4, 0.8], fraction=0.045*im_ratio)\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "    cbar.ax.set_title('Accuracy',fontsize=16, pad = 12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # plt.show()\n",
    "    \n",
    "def plot_confusion_matrix_sns(y_true, y_pred, classes):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # convert to percentage and plot the confusion matrix\n",
    "    cm_pct = cm.astype(float) / cm.sum(axis =1)[:,np.newaxis]\n",
    "    sns.heatmap(cm_pct, annot=True, fmt='.3%', cmap='Blues', linewidths=2, linecolor='black') #cmap='Blues'\n",
    "    plt.xticks(tick_marks, classes, horizontalalignment='center', rotation=70, fontsize=12)\n",
    "    plt.yticks(tick_marks, classes, horizontalalignment=\"center\", rotation=0, fontsize=12)\n",
    "\n",
    "    plt.ylabel('True label', fontsize=18)\n",
    "    plt.xlabel('Predicted label', fontsize=18)\n",
    "    \n",
    "    \n",
    "## Define function to get the confusion matrix and print out the plot as well\n",
    "def conf_matrix(y_true, y_pred, le):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # convert to percentage and plot the confusion matrix\n",
    "    cm_pct = cm.astype(float) / cm.sum(axis =1)[:,np.newaxis]\n",
    "    \n",
    "    classes = le.classes_\n",
    "    print(cm)\n",
    "    plot_confusion_matrix(cm_pct, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c9c9a-336e-4c28-82d3-6df8c57f94b0",
   "metadata": {},
   "source": [
    "### Use GPU if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36a76d49-c958-4fc5-b061-c7bd029016aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b346d-e289-4b73-8f9c-e230eed0eb64",
   "metadata": {},
   "source": [
    "### Annotation file\n",
    "\n",
    "- Vision acoustic spatio-temporal registration has been done in our previous work. \n",
    "- Each frame (timestamp) corresponds to a robot TCP position.\n",
    "- The duration for each frame is 0.033 seconds (sampling freqeuncy 30 Hz.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1242f763-f544-46ac-b161-3da92dd3357b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>audio_file_name</th>\n",
       "      <th>image_file_name</th>\n",
       "      <th>class_ID</th>\n",
       "      <th>class_name</th>\n",
       "      <th>class_v2</th>\n",
       "      <th>class_ID_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sample22_0000.wav</td>\n",
       "      <td>sample22_frame000000.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>Laser-off</td>\n",
       "      <td>No defect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sample22_0001.wav</td>\n",
       "      <td>sample22_frame000001.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>Laser-off</td>\n",
       "      <td>No defect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sample22_0002.wav</td>\n",
       "      <td>sample22_frame000002.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>Laser-off</td>\n",
       "      <td>No defect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sample22_0003.wav</td>\n",
       "      <td>sample22_frame000003.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>Laser-off</td>\n",
       "      <td>No defect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sample22_0004.wav</td>\n",
       "      <td>sample22_frame000004.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>Laser-off</td>\n",
       "      <td>No defect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_index    audio_file_name           image_file_name  class_ID  \\\n",
       "0             0  Sample22_0000.wav  sample22_frame000000.jpg         3   \n",
       "1             1  Sample22_0001.wav  sample22_frame000001.jpg         3   \n",
       "2             2  Sample22_0002.wav  sample22_frame000002.jpg         3   \n",
       "3             3  Sample22_0003.wav  sample22_frame000003.jpg         3   \n",
       "4             4  Sample22_0004.wav  sample22_frame000004.jpg         3   \n",
       "\n",
       "  class_name   class_v2  class_ID_2  \n",
       "0  Laser-off  No defect           0  \n",
       "1  Laser-off  No defect           0  \n",
       "2  Laser-off  No defect           0  \n",
       "3  Laser-off  No defect           0  \n",
       "4  Laser-off  No defect           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_df = pd.read_csv(ANNOTATIONS_FILE)\n",
    "annotations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e81a204-f684-498b-a9b6-c5af79ea5ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1889 2413  844  389   80]\n"
     ]
    }
   ],
   "source": [
    "# Get the labels and count the number of samples for each class\n",
    "labels = annotations_df['class_ID'].values\n",
    "label_counts = np.unique(labels, return_counts=True)[1]\n",
    "print (label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2883e3-6ae0-43a0-bf68-5a085dc55b64",
   "metadata": {},
   "source": [
    "### Basline accuracy\n",
    "The baseline accuracy in this multi-class classificiation problem is **43%**, which is the accuracy of trivially predicting the most-frequent class (classify to the largest class– in other words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b09d68-fe7f-4516-857c-e3124f9656e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class_ID_2\n",
       "0    42.0\n",
       "1    58.0\n",
       "Name: class_ID_2, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get the percentage distribution of each category\n",
    "round(annotations_df.groupby('class_ID_2')[\"class_ID_2\"].count()/annotations_df.shape[0]*100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f747b1-1974-49c7-b880-aa8f5011600e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure dataset_2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjYAAAGoCAYAAABVMq+bAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABFGUlEQVR4nO3dd7gdVb3/8feHhAABUmgiioZ2QVAJTQUVAggIUhRFig1RAZUieFFAwIAiRaWLiF6MjYuICIafgBRDkV69AqHHIFXghBCSUJLv74+1Nhkms8/Z+/QzfF7Ps599zpo1M2tmz8z+7rXWrFFEYGZmZlYHiwx0AczMzMx6iwMbMzMzqw0HNmZmZlYbDmzMzMysNhzYmJmZWW04sDEzM7Pa6PfARtIUSQN2j7mkSZJC0rhC2ricNmmgypXLMaD7prdIWkPSnyQ9lffrjIEu01Anac+8L/fs5vwT8/wTerVgQ1yz/SppmqRpA1Qmf1ZmPdCtwCafdMXXy5L+I+kOSb+QtK2kYb1d2LzuAbvg9FRVUFU3+XO/CNgOuAQ4Gji+hfmi1aAuHwPF4+9VSc9J+j9Jv5G0i6QRTeadWHH8ll9Temt9ZgNlKF1vhlJZB4vB8oN8MBrew/mPzu/DgDHAOsDngC8Bt0n6TEQ8UJrn88DIHq63Jw4jfdE+PoBlaGag901vWAVYG/h5ROzdx+s6FZhBCtBHAWsCnwA+Czwo6XMRcXOTea8BpjSZNq0P1tdTfwJuAp7s5vxnAOcB03utRPW25UAXwMy6p0eBTURMLKdJegtwOrALcKWkDSPimcI8A3phjYgn6f6XQ58a6H3TS1bK70/0w7pOiYhpxQRJo4HvAfsDl0v6QERMrZh3StXx24fr65GIeAF4oQfzPws823slqreIeHigy2Bm3RQRbb+ASLM2nb4I8Lec75TStCnleQEBXwBuAP4DzAUeAy4Hds15JjTWW/GaVCrbFGBF4Bekmpl5wJ55+qScZ1xhnnGN5QBrkZpSngdeAq4Htq7Yxol5ngkV015fXnmfVbymdbZvCvtzX+BWYFYu163AV4FFmnw+U4DlgLNJgdzLwD3AF7vxeW8A/BF4Ji/nX8CZwFurjouK18SeHlOlvNPKn2FFnl/lPBc1+dy6LFNvrK9J3t1z3pOaTF8M6ACeAobntD3zPHuW8r4X+N9cxpdJ588dwCnAoi0er1sCl+Vjfi7wAKlWc3RF3il5OcOBw4EH83ofA04ARnTj2DoVuLuw/geBHwNje3reFaatDvwh79eXSNeaj3WyX6dRODdLn82hwD+A2cBM4Drg0908ry4DXszLuRLYuNk2Ah8Hfps/n5dI14LbgQMoXQdo7XrT7r4fkdd1R96Ps/N+uhj4SEX+tUjX1MfyMfI0cC6wZrtl7WQfvv755c/zhrxvOoALgDWazDeSVHt/V2Ff3gjsXpF3Ql7HROB9wP/L+6vTa0Jh/mGk6/ffST9O5gAPkb6f1ijkWwk4Kud7CniF9APxXOBdTc6Dqlf5WN4G+Avph83LwMPAD4ExTcq7TS7DS3k7Lyp8lpXbDHwauLawff+X9+9iFXmn5dco4KT896t5m47P6/h8J+dMAJM72+c9bYqqFBHzJX2fdEDsLumgyKVq4ljSTngUOJ+0c94KbESq+fk9aeOPBr6R5zmlMP9dpeUtQ6q2nwVcCMwnnVRdWYV0cP8T+Fkuw67ApZL2iIjft7CMZo4mXZjWZUGTBoX3zvwG2IN0gfgF6YP9BCm4+BDwmYp5xpAOzldIJ/jiwKeAcyTNj4hftVJoSduTghrl5fyLdHB9FdhJ0gdjQS3G0aQvly/wxqaeKfS/o0lNe9tLGhURMwfR+v5EOsY/I+lbEfFaafpOpM/vxxXTXifpvcDNpOPhz6TzZxTpS/xrwBGkC0ZTkvYBfkq6iP2BFLxOAL4N7JA/3xkVs54LfBi4lPSlvB3wLWAF4IudrbPkK6Rj+RrSF/swYH3gYGBbSe+PiBfbWN5CJK1BOq+XzeW9i7SPLsr/t7qcEaQfW5sBU4GfkL4gPwX8XtL4iDi8xWVtQtreEaRr1EPAeNK5cnWT2Y4nXctuJv1gGw1sQbqebETqBtDQyvWm3X0/iRSU/xP4NekLbCXSNeijeRmN7fto3q5Fgcl5+94O7Ax8TNLmEXFHG2Xtys7AtqRzawppX34S2FzSJhFxf6FsY0j7eD1SkHYO6cfjNsC5ktaJiCMq1rEx6Xvq+jzPcqTra1P5mPl/wEdI1+9zSefLONK+v54UTAJsSgqa/0a65s4C1iAdXzvmc/HunHcK6RpxICkwvaiw2rsK6z+KtH+fJ/V5fIb0Y+i/ge0kbVy8VknaNZfxZdJ38ZPAJqTzp7Hu8jb+IO+XZ/O8s0ifxQ+AbSRtFRHl69AI0mewDPDXvE8eJR2LhwD7kI6xsn3y+8+qyvK6dn9lFCPsLvIsRrqoBrBKIX1KeV7gOeDfwMiK5SxXFe11Vba8U4ZXTJ9E8xqbAH5Yyr9h3o4OYFRFxDyhYh2N5U3qat2l6VX7pvHr/g5gqUL6ksBtedoeTfbBL4BhhfS1gdeAe1v8nJciHazzgA+Xpn07r+OvpfQJtFkj0uoxVToGuvy1RLqQBLB5xec2Jf9d9fpAb62vk7w/y3m3r5j2//K09xTS9qT0a4z0yzqAnSqWMZbCr/iq4xV4J+kCNhNYqzT/mTn/2VXHKKmmYJnS8fhQPlZWbONzf2fxGC2kfymv59ul9IW2o4Xz7q85/cBS+k4sOFf2LE2bRuk6Q7p4B+nX7/BC+gqFY2STFrZZpMBooc+O9EXVKNOE0rTVKpa1CAtqC99fmjaps+O2nX1PCqLmk645VfMsWzr2OkjXjrVL+dYhffHd0U5ZO9mXjfNioXOpsC+varKub5XSFyfVoM0HxhfSJxTWsU+b5fsBC354LFaathiwfOk4WrpiGevmfXZpK8d7YfrmefoNlGpnCvvt5ELa0vlzexlYt5S/UZNS/u7cOKdNp3Dek2p0J+dph1ecW0EKhJesKPcllK5/OX0pUu3m9Kpj8A152/mQCito6UuIVJ0WwPsKaVPK85ICm0fLH3yTZU6j68DmZWCFJtMXOoEKB8iMJgdWY54vFNIm0j+BzRV5nqrmsC3ztKsr9sFLFAKxwrRr8vSFtrMi72dy3nMrpg3Pn1kA7yikT2DwBDY35XyfLqQ1PrfOXt/orfV1kneTnPcPpfQVScFn+cK/J80Dm4WOjYr1LXS8At/JaT+oyD+WFPDMoXBesiCwqWp6OJomwVq7L9KX/wsVx3Zb5x2pliCAR6j+Qm5sz56l9GksHNg8SPrSW6tiOY1g4JwWtu2DOe81FdOGkQLEym1ssrz1c/6jSumTWjluW9n3pJrAINUCq4v5D8x5v95k+sl5+tq9UNbGeXFVxbTivnxnTls2n1+3Nlneujn/iYW0CTntzjbLNoz0nTIbWKmH58OfSU2FxeblhY730jx/ytPXaTL9TuCZwv+fbXYMk4KKjvJnBPw8p+1dMc9/kX7oPFJKn5bnWbdJuT6Wp59eSt+n6jivevVJU1SB8nt0ke93pM6X90j6A+nL98ZIHSa7Y1oUOiy34Y6orvaeQmpeWY/066g/rU+6mE6pmHYN6cBZr2Lag1HdHPJYfh9Din67WjdUVI1HxGuSriWdXOsxOO+26ez4Ozra7zzck/W9QUTcIOkBUnPP2IjoyJM+Q7ogTmphfb8nfYlcJOkC0i+gv0frHV87+3w7JN1Jqh5fi4WroW+rWF7j2BrbSJD0DdKxVnRRRNyVpy9KumDtRqpRHM0bh6F4W9eb0anGuXF9RMyrmD6F1LTUKUlLk5qvHo/qzuGNfVh1LpY19vs15QkRMU/S9cBqFWVYllRNvx2wKqmWrKitfdXOvo+ImZImAzsAd0n6I6lv0c0RMbu06I3z+7qSJlas+r/y+7uAe9spcye62pfrkZrRNyKdX9GkbIsWylZ2S5tlWou0T2+OiJZuppD0MVJ/nA1JTV3l7+jlaP3ml41JrQ27SNqlYvoIYHlJy0bEcxTOlXLGiJgl6S5SkFfU2TXkAUn/BlaRNCbe2KQ9l9RPrcqlpB/Nn5P07cLxtTfp++4XTeZ7XZ8FNpIWJ7WfQerQ2JmDSB2a9iK1MR4KvCbpL8A3I+KhNlf/VJv5G5r1w2ksb3Q3l9sTo4HnI2KhttwcXDxLqsIsm9FkeY0+G62MM9TY3mYnUiN9TAvLGgiNO7S6Ov4Gan2/IvUv243UzwVSAP0qqUNwpyLiFkkfJtW8fIrcx0LS/aTAratldPvzjep+N1XH1jdITR5F01jQD+D3pL4Gj5A6oT5FqnFtzLtYk7K1qrGNXZ3brS6nN86FtsuU+4XcSuoHeAupqf150j4fQwpw291X7e77XUlN0HuwYKiPuTmo/u+IaGzPsvn9K12sf6k2y9uZVq/djbJtlF/NVJWt3e+VMfm9paFFJB1A6mPUQaqpn06q7QkW9EFq5zNelvQd/90u8i1FajXp6risSm/lvHhHzjejkP5M5GqYskh9dH9Gav7aFfilpA1IQdRFrQSJfVlj86G8/KejdItsWf4ldSpwqqQV8ry7kToOr5M7c73c2TLKi+xekXlLk/QV83uxBml+fq/ah2O6uf4qLwDLSFo0Sh2wJA0nRfB91TG2sb0rNpn+1lK+QUPS6qRmiNdI/UEG4/p+Q7pV/AvATyWtB7wHuDgiWgqOIuJGUoflxUiduj9Kqv08V9J/IuLKTmYvfr73VEzv8ecbEeOaTZO0IemL9Upgu+LxLWkRUmfksnbPu0bZuzq3u9Kb50J3yvRlUlCzUE2jpI1JgU3LurPvI2IOuR+apJVJtXl7kpowxpE6k8OC7Vs3Ipr9Ku9trV67G+8nR8TBba6j3e+VGfm9y5q0fC0/mhQ8rR9pWJLi9I0rZ+zcC6R+dst0mTNpfI8025dV6cXzoqqmuNl50dW+PIe0P/YBfkmrnYazPnmkQj4xvpP/PbedeSPimYi4MCI+TareWg14dyHLPFqrbeiO9XOVc9mE/H5nIa3RdLByRf4Nmyy/URXeTvnvJH1Om1ZM2zQv646Kab2hsb0TyhPyifih/G9frb8njsrvk5s0Lw74+iLiMdIx/n5Ja5ICHOhGc2dEvBwRN0TEUaRbciF1ju1MZ5/vGNKdJXOB+9otT4tWz+9/LgftpNtql6iYp93zrrGNH1L1aOgTuiokQP5MHwbelu+yKts8v7dyLjTyLNQElsv4oXI6C/bVHyumNWtK6+x60519/7qIeCwifke6k+hB0v5t1IbclN8/XDlz+2VtRVf7snEc3EIKjtspW3dNJQU375W0Uhd5lyMF5jdUBDVLsaDJp6irfXYTMFbSOi2W9/VzpTwhl2F8J/NMqJin8WPv0SY1vE3lH3YXkK6NHyTdRDONdCNAl3o9sMk1LueRNnQ6qVd4Z/kXk7SlJJXSF2VBU1axDfc5UrtgpydeN41mwRdUoxwbkvo9vEDqjNXQaG/9Yv6Sb+RfubyMgufy+zvaKNM5+f04Sa+PSpz/bjyq4H/aWF47LiJVd+8u6QOlad8gtfNfGYNoYEFJoySdRmqWmUFq1hzM65uU379EOnmfI90V0Mq6P5wHCCxr/LIq930o+y2p2Wv/fBEq+h6pw+hv26wtbce0/D6hmJivIT9pMk9b511E/JtUrb8KsF9pPTvRQv+agnNI/ah+WAySJC0HHFnI05UbgPuBTXMZivajon8NzffVeqS7tap0dr1ptrzKfS9peUnvr1jOkqS7aV5jwa3PvySdC9+V9L6KZS0iaUIpuTvXxqItlIamKGrsy79FxL8g/XAm9encUNKRxWOoUL7VJK3SzXK8LrdEnEkKEs/KtarF9YyQtHz+9xnS+bpBDiIaeRYltWYsV7GKRmfeZvvs5Pz+86rAStKSpev6xSwYhmLdUvYjqK4RbRzvRxS2pRFU/ogUY3T3+6nRPP97UnPZ2RExv5P8r+tRU1Sh89UiLHikwodInZJuAT4TacTTzixBqg6dJulmUgevxYGtSB24/hwRxV+MV5HaRi/LnVdfBu6OiMk92ZbsWuDL+QT+OwvGsVmEdJvf600+EXFzXv+mwC2SriZ9oexAGuui6hflVaTOfz/P7dKzgBkRcUazAkXEufni92lS5+qLWNDmugpwfv7l1Otyh7G9SOObXKPUsXs6qclja1K16T6dLKJt6vy5J18rdVT8htIDNsWCRxxsSrrYPgB8NhZ+pEfDhCadByF9JqdUpPdkfc1cSKoC/gap4+LpFb+gm/kmsLXSs60eIR1P65DGkOggDc7YVERMU+rc+xPgDknnk/oHbUbqeDiV1Keir9xKOs92lnQDqdPiW0jlv5+K0au7ed59nTQOxymStiZ1hF6d1BTT6BDbih/lsu0E3J37AI4kNZmvQLqTZqGOlxXbEJK+RAq4/iipMY7NuqTxTi4jNSkW/Zp07ThF0uakWpI1gO1Jx9CuFavq7HrT7r5/G3CTpPtINU6Pkc6B7UnNEKc1aioj4jlJnyI/BkTSVaSmzvmkL+GNSf0/Fm+xrK2YDPxJ0p9YsC+3I/0w+1op736kfXcMqYPq9aT+IyuRvnM2Iv3IeLTFdXfmaOD9pGPsAUmXkG7aWJl0DT2EdFfT/PwD6VDg/yRdTPoe3Zz0A/9vLKgVBF6/Pt8MfFjS70jXoHmk78x/RMRVkg4FjiM98uUveZuWIvV724z0uX80L2+mpK+RfvDckK8HjXFs1iV10N6MBc3BjZsgTiQ1Xf4zf3YvkY6jd+fl/7A7Oy4i/i7p7rzuV2ntR8PrM3fn1rMovV4mjVlwO+n2r49SMSJunncKhdt6SRfzb5F6Qk8nVX3/h1SNti+lkUxJXyI/JY178xoL394ZpOHym5V9Es1v955EOrAvZsHImn8HtmmyrDF5exsj8v6T1HN7XLlchXkOJlXtv5zzTGu2bwrpi5BOzttymWbnff31qv3c2T6o2v4WPu+NSBep/5B+lU3Pn8FCtzDSw9u9u3iNiTfeLth4vUq6gP0fqd/Kp8rHTWE9E1tYz7TSPN1eX4vb/ovCsjdokmdPFr7de2vSr+N7Sb+0XiJ9KZ1Gvr21YrsnVCx7a1IVb0c+Lh8CTqRiZNJmx2izMraw7cuQftVOI537D5NqeUfSfPTfMbR/3q1OqtqekffTjXRv5OHFSSMu/5N0K/yLpIv3QiPWtrDtxZGHX6TrkYfXJt32+0zehttJfW862+7Orjct7/u8z48iNZ0+npf3ZD4edqfiFvBcrjNIQdhcUgA/lXTOfLydsnayD1///EhB1o1538wgNdv9V5P5RpACnBtI587LpOvaVaQfGcVxeSbQjWtaYf7heV23sGDk+AdJPzxWL+U7mHQ+zyH9cPwNKQiZRMV1m3RcTybVeM2n+lj+EGmwvSdI1+//kDrvnwRsWFHebfN+mU26JlxMusOrMb5M1XVhN9J58GL+rO8hdUdZvCLvG46tLvbdgXmdf2glf+OlPLOZmdmQImlPUnD/xYiYNLClqa/ctPQIaUyrVjvc98Z6J5H6Hn4kIq5qdb4+6TxsZmZmQ4ukMcW+nDlNpD427yA1e/ZXWVYm1QTdR/PHjFTq6wH6zMzMbGj4AOm5Z38lNRktldPGk/pVTezrAkjagzSI426kcXuOjDablhzYmJmZGaQ+epeQHvuxHSlG+Dep394Ponsj+rdrb9LNAY8BB0VE1RAHnXIfGzMzM6sN97ExMzOz2nBgY2ZmZrXhwMbMzMxqw4GNmZmZ1YYDGzMzM6sNBzZmZmZWGw5szMzMrDYc2JiZmVltOLAxMzOz2nBgY2ZmZrXhwMbMzMxqw4GNmZmZ1YYDGzMzM6uN4QNdgKFM0p3AKsAs4KEBLo6ZmdXP6sBSwKMRsd5AF2YoUEQMdBmGLEkzgNEDXQ4zM6u9FyJizEAXYihwjU3PzAJGjx49mvHjxw90WczMrGbuuusuXnjhBUjfN9YCBzY98xDwtvHjxzNlypSBLouZmdXMhAkTuOaaa8DdHVrmzsNmZmZWGw5szMzMrDYc2JiZmVltOLAxMzOz2nBgY2ZmZrXhwMbMzMxqw4GNmZmZ1YYDGzMzM6sNBzZmZmZWGw5szMzMrDb8SAUzG7IkDXQRzBbih0sPLNfYmJmZWW24xsbMhrxtjjhvoItgxuXf322gi2C4xsbMzMxqxIGNmZmZ1YYDGzMzM6sNBzZmZmZWGw5szMzMrDYc2JiZmVltOLAxMzOz2nBgY2ZmZrXhwMbMzMxqw4GNmZmZ1YYDGzMzM6sNBzZmZmZWGw5szMzMrDYc2JiZmVltOLAxMzOz2hh0gY2kT0m6QdJzkuZKul/SEZJGFPJI0uGSHpM0R9K1ksZXLGttSVdJmi3pCUnHSBpWytPSsszMzGzwG3SBDbAs8Dfgy8C2wDnAd4CTCnkOBY4ETgB2AGYBV0pasZFB0ljgSiCAnYBjgG8CR5fW1+WyzMzMbGgYPtAFKIuIn5WS/iZpFPB1SfsDi5GCkeMi4gwASTcC04D9gCPyfPsCSwA7R8RM4Iq8nImSToyImZIWb3FZZmZmNgQMxhqbKs8BjaaoTYBRwPmNiRHxEjCZVMPTsC1weQ5qGs4jBTubtbksMzMzGwIGbWAjaZikkZI+BBwA/DQiAlgLmAc8WJrlvjytYS1gajFDREwHZhfytbosMzMzGwIGbWADvJRf1wHXAIfk9LHArIiYV8rfAYwsdDIeC8yoWG5HntbOst5A0t6SbgM2aH1zzMzMrK8N5sBmE+DDpA6/OwFnFKZFRX5VTGuWr5U8zaYREWdHxIbA7VXTzczMbGAMus7DDRFxR/7zeknPAr+S9GNSbcrSkoaValrGALMj4tX8f0dOKxvNgpqcVpdlZmZmQ8BgrrEpagQ5q5D6zQwDVi/lKfepmUqpn4yklYElC/laXZaZmZkNAUMlsPlgfn8UuAGYCezSmChpJGkMmksL81wKbCNp6ULarsAcUp8d2liWmZmZDQGDrilK0mWkgfXuId2x9EFSP5vfR8TDOc/xwJGSOkg1KweTgrTTC4s6i3Q31YWSTgBWBSYCJzVuAY+IuS0uy8zMzIaAQRfYALcCewLjgNeAR4DDSIFKw/Gk4OMw0kjFtwFbRcTTjQwR0SFpS1Kn48mkfjUnk4Ib2lmWmZmZDQ2DLrCJiCNJjzjoLE8Ax+ZXZ/nuBbbojWWZmZnZ4DdU+tiYmZmZdcmBjZmZmdWGAxszMzOrDQc2ZmZmVhsObMzMzKw2HNiYmZlZbTiwMTMzs9pwYGNmZma14cDGzMzMasOBjZmZmdWGAxszMzOrDQc2ZmZmVhsObMzMzKw2HNiYmZlZbTiwMTMzs9pwYGNmZma14cDGzMzMasOBjZmZmdWGAxszMzOrDQc2ZmZmVhsObMzMzKw2HNiYmZlZbTiwMTMzs9pwYGNmZma14cDGzMzMasOBjZmZmdWGAxszMzOrDQc2ZmZmVhsObMzMzKw2Bl1gI2kXSX+W9LikWZJul7R7Kc80SVF6PVWxrLUlXSVptqQnJB0jaVgpjyQdLukxSXMkXStpfB9vppmZmfWB4QNdgAoHA48CBwHPAtsB50paLiJOL+Q7Fyj+/0pxIZLGAlcC9wI7AasBPyYFc0cUsh4KHAkcAkzN679S0rsjYqFgyczMzAavwRjY7BARzxb+v1rSSqSAoxjIPBkRN3WynH2BJYCdI2ImcIWkUcBESSdGxExJi5MCm+Mi4gwASTcC04D9eGMAZGZmZoPcoGuKKgU1DXcCK7S5qG2By3NQ03AeKdjZLP+/CTAKOL+w/peAyXl+MzMzG0IGXWDTxCakJqWivSS9IukFSRdIemdp+lqkpqXXRcR0YHae1sgzD3iwNO99hTxmZmY2RAzGpqg3kLQlqY/MXoXki4GbgH8D7wK+C1wn6T0R8ULOMxaYUbHIjjytkWdWRMyryDNS0oiIeAUzMzMbEgZ1YCNpHKmT8MURMamRHhEHFrJdJ+kG4C7gi8AphWlRtdhSerM8zaYhaW9gb2DNzspvZmZm/WvQNkVJWga4FJgOfLazvBHxT+B+YP1CcgcwpiL7aBbU5HQAS5dvAc/zzY6IV5us7+yI2BC4vdONMDMzs341KAMbSSOBS4ARwMdyh95WFGtYplLqJyNpZWBJFvS9mQoMA1YvLWeh/jlmZmY2+A26wEbScOAPwBrAthHxTAvzvJvULFSsQbkU2EbS0oW0XYE5wDX5/xuAmcAuhWWNBHbI85uZmdkQMhj72JxJGpTvQGAZSR8oTLsT+AipaeoS4AlS7coRpCarSYW8ZwEHABdKOgFYFZgInNS4BTwi5ko6HjhSUgcLBuhbhDeOmWNmZmZDwGAMbLbO76dWTFsFeIw0ps0ppL4wzwGXAYcXx6yJiI58R9UZpHFpZgAnk4KbouNJgcxhwLLAbcBWEfF0b2yMmZmZ9Z9BF9hExLgWsm3Z4rLuBbboIk8Ax+aXmZmZDWGDro+NmZmZWXc5sDEzM7PacGBjZmZmteHAxszMzGpj0HUefrOT1HUms36U+tebmQ0NrrExMzOz2nCNzSB16wYbDXQR7E1uo9tvHegimJm1zTU2ZmZmVhsObMzMzKw2HNiYmZlZbTiwMTMzs9pwYGNmZma14cDGzMzMasOBjZmZmdWGAxszMzOrDQc2ZmZmVhsObMzMzKw2HNiYmZlZbTiwMTMzs9pwYGNmZma14cDGzMzMasOBjZmZmdWGAxszMzOrDQc2ZmZmVhsObMzMzKw2HNiYmZlZbTiwMTMzs9pwYGNmZma14cDGzMzMasOBjZmZmdXGoAtsJO0i6c+SHpc0S9LtknYv5ZGkwyU9JmmOpGslja9Y1tqSrpI0W9ITko6RNKw7yzIzM7PBb9AFNsDBwCzgIGBH4G/AuZL2L+Q5FDgSOAHYIee/UtKKjQySxgJXAgHsBBwDfBM4urS+LpdlZmZmQ8PwgS5AhR0i4tnC/1dLWokU8JwuaXFSMHJcRJwBIOlGYBqwH3BEnm9fYAlg54iYCVwhaRQwUdKJETGzjWWZmZnZEDDoamxKQU3DncAK+e9NgFHA+YV5XgImA9sW5tkWuDwHNQ3nkYKdzdpclpmZmQ0Bgy6waWIT4N7891rAPODBUp778jQK+aYWM0TEdGB2IV+ryzIzM7MhYNAHNpK2JPWR+UlOGgvMioh5pawdwEhJIwr5ZlQssiNPa2dZ5TLtLek2YIN2tsXMzMz6VluBjaRHJO3YyfTtJT3S82K9vrxxwLnAxRExqTApqrJXTGuWr5U8zaYREWdHxIbA7VXTzczMbGC0W2MzDliqk+lLAu/sdmkKJC0DXApMBz5bmNQBLF2+bRsYA8yOiFcL+cZULHo0C2pyWl2WmZmZDQG93RT1FlIflh6RNBK4BBgBfCx36G2YCgwDVi/NVu5TM5VSPxlJK5OCr6mFPK0sy8zMzIaALm/3lrQpMKGQtLOkciAAsAywG3BXTwokaTjwB2AN4IMR8Uwpyw3ATGAX4Pt5npGkMWjOLuS7FDhE0tIR8WJO2xWYA1zT5rLMzMxsCGhlHJvNge/mvwPYOb+qPEQaWK8nzgS2Aw4ElpH0gcK0OyNirqTjgSMldZBqVg4m1T6dXsh7FnAAcKGkE4BVgYnASY1bwNtYlpmZmQ0BrQQ2pwCTSB1qHwG+AVxcyhOku4ue74UybZ3fT62Ytgpp8LzjScHHYcCywG3AVhHx9OsFiujId1SdQRqXZgZwMim4KepyWWZmZjY0dBnYRMQLwAsAkjYH7qtoHuo1ETGuhTwBHJtfneW7F9iiN5ZlZmZmg19bj1SIiGu6zmVmZmY2MNp+VpSkdwD7kDr3LsuCMV8aIiK27IWymZmZmbWlrcBG0rbAn0i3Yb8I9EafGjMzM7Ne0W6NzXHAs8DHI+K2PiiPmZmZWbe1O0DfWsApDmrMzMxsMGo3sPkP8EpfFMTMzMysp9oNbH4DfLIvCmJmZmbWU+32sZkEbC7pYtIAeo8C88qZImJ6z4tmZmZm1p52A5uppFGGBWzfSb7y07LNzMzM+ly7gc0xpMDGzMzMbNBpd+ThiX1UDjMzM7Mea7fzsJmZmdmg1e7Iw5u2ki8iru1ecczMzMy6r90+NlNorY+NOw+bmZlZv2s3sPlik2WsBuwJTAN+1rMimZmZmXVPu52Hf9VsmqQfAnf0uERmZmZm3dRrnYcjogP4BfCt3lqmmZmZWTt6+66oDmDVXl6mmZmZWUt6LbCRtDjwOeCp3lqmmZmZWTvavd37nCaTlgE2BpYHDulpoczMzMy6o927ovZskv488ABwUESc26MSmZmZmXVTu3dFeaRiMzMzG7QcqJiZmVlttNsUBYCkUcBHWHAH1CPAFRHxYm8VzMzMzKxdbQc2kr4M/BhYClBODmCWpIMj4n96sXxmZmZmLWv3rqgdgbNJNTRHAf/Mk9YB9gfOlvRMREzu1VKamZmZtaDdGptvAfcB74+IWYX0qyT9ErgJ+DbgwMbMzMz6Xbudh9cFJpWCGgBy/5pf5TxmZmZm/a47d0Wpk2nR3YKYmZmZ9VS7gc3dwBckLVmeIGkp0gB+d/dCuczMzMza1m5g8yPgXcAdkr4uafP82g+4HVgL+GFPCyVpdUk/k3S3pHmSplTkmSYpSq+FnlMlaW1JV0maLekJScdIGlbKI0mHS3pM0hxJ10oa39PtMDMzs/7V7sjDF+Ug5gTgdBY0PQl4CdgvIi7uhXKtA2xH6ow8opN85+ZyNLxSnChpLHAlcC+wE7Aa6Vb1RYAjClkPBY4kPedqKnAwcKWkd0eEH+ppZmY2RLQ9jk1EnCnpXGArYBVSUPMwaYC+F3qpXJMbAZKkC4DlmuR7MiJu6mQ5+wJLADtHxEzgijy44ERJJ0bEzPxU8kOB4yLijLzOG4FpwH68MQAyMzOzQaxbIw9HxAzgD71blDcsf34vLWpb4PIc1DScR6px2ox0W/omwCjg/ML6X5I0Oc/vwMbMzGyI6LKPjaRhko6XtG8X+b4q6QeSOrtrqrftJekVSS9IukDSO0vT1yI1Lb0uIqYDs/O0Rp55wIOlee8r5DEzM7MhoJXOw58l9T25tYt8t5AG59u9p4Vq0cXA14EtSeXbGLhO0uhCnrHAjIp5O/K0Rp5ZETGvIs9ISQv18ZG0t6TbgA16tAVmZmbWq1oJbD4NXBkRt3eWKU+/nH4KbCLiwIj434i4LiLOBrYBVgK+WM5aMbtK6c3yVE6LiLMjYkPSnWBmZmY2SLQS2GxAurOoFX8DNux+cbovIv4J3A+sX0juAMZUZB/NgpqcDmDp8i3geb7ZEfFqrxbUzMzM+kwrgc0ywDMtLu8/Of9AKtawTKXUT0bSysCSLOh7MxUYBqxeWs5C/XPMzMxscGslsHmR5rdbly0LLPQcqf4g6d3AmryxeehSYBtJSxfSdgXmANfk/28AZgK7FJY1Etghz29mZmZDRCu3e98DbE0a2K4rW+X8PZIDi+3yv28DRkn6VP7/L8DmpE7NlwBPkGpXjgCmA5MKizoLOAC4UNIJwKrAROCkxi3gETFX0vHAkZI6WDBA3yK8cfA/MzMzG+RaCWwuBH4saafORhWWtCMpsDm4F8q1AguPk9P4fxXgsZznFFJfmOeAy4DDi2PWRESHpC2BM0hj1swATiYFN0XHkwKZw0i1TrcBW0XE072wLWZmZtZPWglsfgZ8FThf0o+An0fEtMZESeOALwP/DTyQ8/dIXn5X4+Fs2eKy7gW26CJPAMfml5mZmQ1RXQY2ETFH0sdIzT6HAYdKepHUL2Vp0qi9It2RtH1EzO3D8pqZmZk11dLTvSPiIWA8cCBwPfAasCJpxN7rcvr6EfFw3xTTzMzMrGstPysq18ScjjvUmpmZ2SDVUo2NmZmZ2VDgwMbMzMxqw4GNmZmZ1YYDGzMzM6sNBzZmZmZWGw5szMzMrDYc2JiZmVltOLAxMzOz2nBgY2ZmZrXhwMbMzMxqw4GNmZmZ1YYDGzMzM6sNBzZmZmZWGw5szMzMrDYc2JiZmVltOLAxMzOz2nBgY2ZmZrXhwMbMzMxqw4GNmZmZ1YYDGzMzM6sNBzZmZmZWGw5szMzMrDYc2JiZmVltOLAxMzOz2nBgY2ZmZrXhwMbMzMxqY1AGNpJWl/QzSXdLmidpSkUeSTpc0mOS5ki6VtL4inxrS7pK0mxJT0g6RtKw7izLzMzMBrdBGdgA6wDbAQ/kV5VDgSOBE4AdgFnAlZJWbGSQNBa4EghgJ+AY4JvA0e0uy8zMzAa/wRrYTI6IlSNiF+Ce8kRJi5OCkeMi4oyIuBLYhRTA7FfIui+wBLBzRFwREWeRgpqDJY1qc1lmZmY2yA3KwCYi5neRZRNgFHB+YZ6XgMnAtoV82wKXR8TMQtp5pGBnszaXZWZmZoPcoAxsWrAWMA94sJR+X55WzDe1mCEipgOzC/laXZaZmZkNckM1sBkLzIqIeaX0DmCkpBGFfDMq5u/I09pZ1usk7S3pNmCDbpbfzMzM+sBQDWwg9YEpU8W0ZvlayVM5LSLOjogNgdtbKKeZmZn1k6Ea2HQAS5dv2wbGALMj4tVCvjEV849mQU1Oq8syMzOzQW6oBjZTgWHA6qX0cp+aqZT6yUhaGViykK/VZZmZmdkgN1QDmxuAmaTbsgGQNJI0Bs2lhXyXAttIWrqQtiswB7imzWWZmZnZIDd8oAtQJQcW2+V/3waMkvSp/P9fImK2pOOBIyV1kGpWDiYFaqcXFnUWcABwoaQTgFWBicBJjVvAI2Jui8syMzOzQW5QBjbACsAfSmmN/1cBpgHHk4KPw4BlgduArSLi6cYMEdEhaUvgDNK4NDOAk0nBTVGXyzIzM7PBb1AGNhExjQV3JTXLE8Cx+dVZvnuBLXpjWWZmZja4DdU+NmZmZmYLcWBjZmZmteHAxszMzGrDgY2ZmZnVhgMbMzMzqw0HNmZmZlYbDmzMzMysNhzYmJmZWW04sDEzM7PacGBjZmZmteHAxszMzGrDgY2ZmZnVhgMbMzMzqw0HNmZmZlYbDmzMzMysNhzYmJmZWW04sDEzM7PacGBjZmZmteHAxszMzGrDgY2ZmZnVhgMbMzMzqw0HNmZmZlYbDmzMzMysNhzYmJmZWW04sDEzM7PacGBjZmZmteHAxszMzGrDgY2ZmZnVhgMbMzMzqw0HNmZmZlYbQzawkbSnpKh47VvII0mHS3pM0hxJ10oaX7GstSVdJWm2pCckHSNpWL9ukJmZmfXY8IEuQC/YAphT+P+Rwt+HAkcChwBTgYOBKyW9OyKeApA0FrgSuBfYCVgN+DEp6Duiz0tvZmZmvaYOgc2tETGrnChpcVJgc1xEnJHTbgSmAfuxIGjZF1gC2DkiZgJXSBoFTJR0Yk4zMzOzIWDINkW1YBNgFHB+IyEiXgImA9sW8m0LXF4KYM4jBTub9UM5zczMrJfUIbB5WNJrku6XtE8hfS1gHvBgKf99eVox39RihoiYDswu5TMzM7NBbig3RT1J6j9zCzAM2B04S9LIiDgZGAvMioh5pfk6gJGSRkTEKznfjIrld+RpC5G0N7A3sGZvbIiZmZn1jiEb2ETE5cDlhaRLJS0GHCHp1Ea2illVMa1Zvqp0IuJs4GxJU3BzlZmZ2aBRh6aooguAZYBxpBqXpStu2x4DzI6IV/P/HTmtbDTVNTlmZmY2SNUtsGkIUr+ZYcDqpWnlPjVTKfWlkbQysGQpn5mZmQ1ydQtsPgk8C/wLuAGYCezSmChpJLADcGlhnkuBbSQtXUjblTQ2zjV9XWAzMzPrPUO2j42kP5I6Dv+DVDOza34dEBHzgbmSjgeOlNTBggH6FgFOLyzqLOAA4EJJJwCrAhOBkzyGjZmZ2dAyZAMb4H5gL2BlUkffe4HPR8RvCnmOJwUyhwHLArcBW0XE040MEdEhaUvgDNIYNzOAk0nBjZmZmQ0hQzawiYjDgcO7yBPAsfnVWb57SY9mMDMzsyGsbn1szMzM7E3MgY2ZmZnVhgMbMzMzqw0HNmZmZlYbDmzMzMysNhzYmJmZWW04sDEzM7PacGBjZmZmteHAxszMzGrDgY2ZmZnVhgMbMzMzqw0HNmZmZlYbDmzMzMysNhzYmJmZWW04sDEzM7PacGBjZmZmteHAxszMzGrDgY2ZmZnVhgMbMzMzqw0HNmZmZlYbDmzMzMysNhzYmJmZWW04sDEzM7PacGBjZmZmteHAxszMzGrDgY2ZmZnVhgMbMzMzqw0HNmZmZlYbDmzMzMysNhzYZJLWlnSVpNmSnpB0jKRhA10uMzMza93wgS7AYCBpLHAlcC+wE7Aa8GNS4HfEABbNzMzM2uDAJtkXWALYOSJmAldIGgVMlHRiTjMzM7NBzk1RybbA5aUA5jxSsLPZwBTJzMzM2uUam2Qt4OpiQkRMlzQ7T5vcZL7VAe666y4mTJjQqwXa6PZbe3V5Zt3V28d2X7j8+7sNdBHMXteb58xdd93V+HP1XltozSkiBroMA07Sq8AhEXFKKf3fwK8j4vBS+t7A3sB4wB2Mzcysr70QEWMGuhBDgWtsFqiK8FSVHhFnA2dLuhNYBZgFPNS3xbNu2AC4faALYTZE+HwZnFYHlgIeHeiCDBUObJIOYExF+mhgRrOZImK9PiqP9QJJt0XEhIEuh9lQ4PPF6sKdh5OppL40r5O0MrBknmZmZmZDgAOb5FJgG0lLF9J2BeYA1wxMkawXnD3QBTAbQny+WC248zCvD9B3L/BP4ARgVeAk4JSI8AB9ZmZmQ4QDm0zS2sAZwMakfjW/ACZGxLyBLJeZmZm1zoGNmZmZ1Yb72JiZmVltOLCxIUmShyowM7OFOLCxIUHSKEm7SdoeICJey+ka2JKZDU75nFky/+0fAvam4cDGhorhwI7AEUreI+lXwCYDXC6zQUfSUsD3gD0kLRIRr0kaNdDlMusPDmxs0JOkiHge+CMwFvgHcHee/OCAFcxskIqIWaRh+L8HvFfSCcDz+e5Ps1pz9aQNWpKGRcS8WHDr3hKkMYZeBb4WEWdJcnBuljWaZiP5kqTZwHXA08CX8fOG7E3AXwo26OSmpkUaYwhJ2kjS24GngGOBh4EPAETEfPezsTe7fM4MzwFN5LSvAIuTfsAeGRGTImLOgBbUrB84sLFBJ1+b50taU9LVwMWkKvWbI2IicDnwAUk751l8HNubVm6qjdyPZmlJh0j6dET8HFgN+Dewu6S3NvIPaIHN+pi/EGxQaVx0JW0CXAa8BOwO/DwiXszZJgH/Ab4iaamImOeLtb1ZFWpo9gceBz4JbCHpXRHxKHAgsD2wdW7e9aisVmseedgGVOPXZkX66cBmwE7AtHIeSd8E9gbOiIjTc9pY4MXGreBmbxaSdgROB04DzgNeyB2IG9P/CiwP7BER9w1MKc36h2tsbEDkPgGVvx4lLUu6jfuWiHi0mEfSsPznb0gPLv2GpE9LOhy4ENigH4pvNiDKneUb/dFIHYOfAX4dEY9HxKzGOZazHgCsC3xc0mhJi0jauH9Lb9Y/fFeU9QtJoyJiZvGuDWCepJVJNTPTgb/nu6Cey+NwDM/zDmt0JC68PyPpTOBbpF+prwJHRcTN/b5xZn2scIfg/GJ6Po9C0urAFRHxn+LdhJIid8SfKukU4GhgLeCdwKaSVsvNVWa14Rob63OSTgOul7Ry6a6NY0nj0HwHmAJcKGmHPNuFpF+Xy5efsC5pgqQREXEF8HHgExGxckT8sp82yaxPSfqApNGN/wt3CO4r6feSJkraLKeNBO4H3pfPi3mNmp2ImN8IhiLiYOAnwNuBR4AVHdRYHTmwsT4jaSlJvwF2Jt1u+lhh2meBXYAvkTo2bkWqdTku95X5JTALOC3/j6RFJb0P+D7wfoCIeCkibuzHzTLrM5I2lDQdOBUYVUhfWdIUYCLpuv1J4HJJe0bEbODvpD40n4UU0OT5Rko6MJ83kGo4d4qIvSLimX7aLLN+5aYo60vLAlsDh0XExZLGAbPzBfWzpMHCLomIF4CHJY0nBUEHRsRESQeTOkKOk3QrMA/4NGnk4Wn9vTFmfUnpuU6nADcDXy8FHtuSApdPAP+IiJcknQgcLmkWcA6wHXCopLuA+0gDWn6c9APiUYCIeJX0A8KstlxjY30it+v/izSGxg6S/kIaAfWtOcsawOUR8YKk1SRdAhwHnAH8GiAizicFOo+SOgV/GPheRGxbrP0xq4llSMHLfbkP2RGFafsC90TEjRHxUk5bFlgdWCU/cuRY0iCWNwO3A/+P1PT014j4c39thNlA8+3e1qckXUqqtfknsB9wA7Ao8DtSkHM1cCgp6DksIm7K820UEbcWljOGdAurD1irLUmTgE1JzVCLAB8C/kUKUu6MiIMkfRU4HngSOBz4U6Hf2ghS8+5bgRGk4RD+3d/bYTaQ3BRlvaLYWbGQ9inSg/imkpqR7sidIOdJugn4NrAmsANwdUS8LGlxYDfgw5Iei4in8nJn9Of2mPWnfFv2W0hNTmOBW4AtIuKVPP0lYCtJDwArAD8Azso1nm+TtAFwfa65+emAbITZIOGmKOuxfHvp/PwYhLcpPdcJ0p1N2wNfBdYBvlCY7c/A9cBM4FZgRB7yfV9SB8fHgRf6axvM+pOkN/yozAH/0qQn2F9ACnI2LGQ5HlibNOL2+yLiBGBm7lh/AKkfjWszzXBTlLWpOKZMKX0p4ExgAvAyKVj5YUTcmavHTyONIrxhRDye53k/8GPSE7ufAuaTanC+ExGn9cPmmA0oSbsBzwH3R8T0nDYBOAF4PCJ2LuT9Fenhr78kPS9teeDrwPrkh1z2Z9nNBisHNtayfIv29qSL6IO5g/D8PFLw70lV6CeQBgDbjNRBeIuIeEjSmsC1wLkRcVBhmUsAOwIrkmoQzy50jjSrJUnbAL8AhpGO+znATyLiR3n6MaQ7B4+MiN/ltGVItZ8HAc8DI0nNvPtGxEP9vhFmg5QDG2uZpL2An5Fuxz6zkL4TcDawK3BdYTCxucBFwAH5Lo//Jt25sXFE3NEIjPp7O8z6mqSVSM1Jc0k1LzML095Baqa9gXQ+vQLsQ/rRcGpE/FTSOsBJpD5q25Se+7QyKahZIiLu6p8tMhs63MfGupSfK7NIRJxDGiH487mzYqPT8DrAjIiYkkc9HS/pGkCkW08bF/XfAncDx0sa7qDG6iYPSnkm8BdSLeY9wB2SflDoV7MNsArwu4i4JyIeJD2xfgXgSEmLRsQ9pP42ywP752U3Oug/FhH3O6gxq+bAxjrVqFUpBCGHkpqaPi5pqZw+Fng1j0fzK+AOUr+B9SPiZNIvUkgP6TuNNB7NuP7cDrO+JukwUufeDUgjB3+T9DDXv5M6xJ+c7/pbHni28VwzSUeRxp25G/hiHkQPUmBzC3CApDX9Q8CsNb7d2zpVGJr968DewKXAksDHSFXplwLnku7MeBC4kzQC6hW59mYccIKkQyJiuqQLSONuuB+N1UIeMfjnpGEKPgNcRhpzqRGI3CRpKnAMacDKfwHL56bZ/UjjzexPejL33PxAy45ID4OdTOp/MxMza4lrbKxLkg4BTiQ1Jd1L6hewLrCrpOVJvzTPAWaTOjJeRnri8MrAV4B3kPobEBFzHdRYzcwmBfUvAI9FREdjQqH56DhSzc3HgbeRBqxsnFPjI+LsHNS8HzgSeF+e7/cR8ZWIeLIft8dsSHPnYetU/jV6DfAAsFdEzM3pRwMHA/tExLmS3gLcBDxNushPI42tsRip8/DkASi+Wb+QtAJp/JlhEfHB0rRhufZyU1Jtzg9ItZ6fJ931dE6+s3A90pPuXwb2851OZt3jGhvrVK5dWRZ4IP+iHJ7Tv0vqT/Dl3P7/NPBRUr+AZYGNgAsiYhUHNVZ3+YGVPwXWlvQleH00YXJQo4i4ltT/bH3SM9F+S3p6/f2kjsaTSeM57eGgxqz7XGNjncq/JP8XeBHYMyJelLR4DnK+BpwO/DdwekS8VphvsYh4eWBKbdb/JI0kDZ73PmDNiHilMNZTo9bmRODLwNsiYk4ez+YdpDui/lp8PpqZdY9rbN6ElAwrp1XljYjnSKMIrwVsldPmlrLtRnpYX3E+BzX2phIRs0k1MYsC38vJytMao3WvRGrWnZfTL4+In0fEsQ5qzHqHA5uaKwcs+Rdk5F+PoyRtIGls1VOzGx0fSbdozwW+nu/YQNLSpPFrJpFuX3WHYLN0p+D5wFckrdFohgKQ9EFSc+1fGw+3zOmVPyrMrHvcFFVzeSC818qj/EqaSHrOzCvAEqQ7nU6LiCdzf4DI+RpV6Z8g3a2xJqkvwDKkh/JtBDztMTbMkhz8Xwg81HjWUx5J+HDSebOX73Iy6zuusakpSatKupI0MBgUnvwr6UjSU7S/TRo/ozGY2FGSlo2IKNTWBEBE/In0a/N/gFGkW1y3iIgnHdSYLZA7/p4DbCHpk5I+B1wJrAYc7qDGrG+5xqamJL0V+B3pduvPRMS03K9mNHAjcAXwzUZfmFyD8znSs2oWerJ2qRZn8Yp+NmaW5QdW/h7YkjTA3lER8eOBLZXZm4NrbGqi8ByaRvPRk6QHU44ljW7a6MA4F3grcGtEvCxpRJ7tRNIYNB/JY9K8QbEPjoMas85FxPOk8+8oYFkHNWb9x4FNDUjaG7hT0ufzbaWNpqELSIPmfVRS466llYD7gQ/mvK9IGpHv6PgjsDGFZisz67YLIuL7/iFg1r8c2AxxktYkPbZgHdKYMj+VtCpAHlfmd+Q7mnLaQ6Th3McDm+XFNB669ywwhtSHxsx6oOpOQzPrew5shriIuJ/0AL77SQ/XewfwN0l75OlXAX8FNsydGCEFQCOBgyWNzp2FxwA7A5d51FMzMxuqHNjUw1+B20gdg/cmDah3pqSzJS1HuuvpSWBPSctExB3AKcB/AdMk/RG4BPgwaeRUMzOzIcmBTQ1ExDTSuBmLAbtHxKeAA0g1MNcD7wQuIjUz7ZdnmwRMAH5Faoq6B1grIi7st4KbmZn1Mt/uXROSRgNHk54YvHFE3C9pM1KAswnpFu8lSf1nPh8RDxbmHV58zpOZmdlQ5cCmRvKdT2cCj0TExwvpRwC7A+/KSedExJf7v4RmZmZ9y01R9XIT8FvSiKfbNBIj4vvAjsAU4GVg6oCUzszMrI+5xqZmJL2L9NDKZSJig5y2aES8mkcjnhsRHQNaSDMzsz7iGpuaiYj7SLU2b5P0zZw8P0970kGNmZnVmWtsakjSSqTh3FcBxkfEq13MYmZmVgsObGoqN0k9HBGvDHRZzMzM+osDGzMzM6sN97ExMzOz2nBgY2ZmZrXhwMbMzMxqw4GNmZmZ1YYDGzMzM6sNBzZmZmZWGw5szMzMrDYc2JiZmVltOLAxMzOz2nBgY2ZmZrXhwMbMzMxqw4GNmZmZ1YYDG7MakzRBUkjac6DL8mbmz8Gs/ziwMSsofAE1e7020GWsO0njJE2UNH6gy2JmQ8/wgS6A2SD1v8BfKtLn93dBeuhaYAng1YEuSBvGAd8FpgF3DWRBzGzocWBjVu2OiPjtQBeiuyQtHREvRsR8YO5Al8f6l6QlgFcjwjWM9qbjpiizbpJ0Ym6e+lwp/b2S5kj6m6RFctrEnHcdSadJeirnuVnSlk2W/xFJf5U0Q9JcSf+QtG9FvmmSpkhaT9Llkl4A/pGnLdS3o5gm6WuS7s/L/z9JH8t53iPpMkkzJT2Xy7xoxbrXkPQbSU9KeiWX5YeSlizlm5TXOVrSTyU9k9f5d0nvL+TbE/hb/veXhSbAKV18Fo39u6akH0j6t6SXJd0tabtS3qb9XRrlLKVNyds1TtKf8ufRkfMuJWkRSYdLejRv0x2SPthJWfeX9EDO+4Ck/Zvka3ffLi/pHElPAy8Bb+9sn5nVlWtszKqNlLRcRforETEz//0dYFPgTEk3RcSDkkYC55G+WD6ba0yKfg3MA04Algb2AS6TtG1EXNnIJGlv4CzgJuDYvLytgJ9KWi0iDikt9x3A1cAfgD8CS7WwjV8HxgK/INXqHABcJGkX4Oek5riLgK2B/YFngO8XyrhBXucM4GfA48C6eTkflLRZRJSbwC4H/gMcAywLHAz8RdK4iHiR1HT2A+Bw4Gzgujzf0y1sD8CvSM1uPwJGAN/I2/RfETGtxWVUWZK0rdcChwIbAXsBiwPPAe8HTgcWBf4bmCzpnXmbivYHViTtrxeB3YHTJC0TEUc3MnVz314BPAV8L5d3Vg+212zoigi//PIrv4AJQHTyuqSUfxXSl8/tpC/S/8n5dijlm5jTbwZGFNLfTvoCuq+Q9lZSoHFuRflOJQVGqxXSpuVlf7mT7dmzIu1xYHQh/b05fT6wc2k5twNPltLuBqYCS5fSP1Gxzkk57cxS3l1y+j6dlbmFz62xfy8BVEjfKKcf18ryG+UspU3J+Q8ppV+Y99VtwKKF9B072aYXgbcX0kcAt5CCsWJ6d/btbwf6/PHLr8HwclOUWbWzSTUk5dd3ipki4lFgb2B90i/svYDTImJyk+WeHBGvFOb/N/A7YC1J78rJnwIWA/5H0nLFFzCZ1IRcbr56Hvhlm9s4KSJeKJTlH8BM4ImIuLCU93pgRUlLQWqqIgVC5wKLlcp4PamGaeuq7S/9f3V+X6PNsjdzakS83pQUEbeSgomeLn8eqUam6DpAwFnxxtqTRi1T1Tp/lz/zRvleIe2T4cAO0KN9+6O2t8qshtwUZVbtwSg0DXUmIs6XtCPwGeCfwLc6yX5fRdq9+X3VPL0R4HS2/reU/n84Iua1UNyiRyrSOoDHmqRDaj6axYIyHp1frZRxoXVGxHOSGsvtDVXb9HwvLP/JiCh3wm7sk0eLiRHR0ck2dfX5Q/f37QNN8pq9qTiwMeshSWOAD+V/VwJWoDo4gNRksNAimvz/eeDJJsspf4HP7ryUlZoFQp0FSCq9/xi4rEnejnJCJ8FXeR90VyvLr/oMGppdEzvbJ+1sUzuff7v7tjvHgFntOLAx67lfACuTOob+EPitpC2afImvTb5jqaDxC70RrDyY359ttdZoADTKOK8PythZ4NEbns/vy1RMW7UirTetXZHW7PPvi31rVnvuY2PWA/n2608C34uIM0h3xGwKHNFkloMkjSjM/3ZgD+D+iGg0U5wPvAwcrTQeSXmdoyUt1oub0R13kprd9pW0UDAgabikqsChFY27ebo7f1ceBV4DPlJMlLQJ8IE+WmfDZ/Jn3ljnCOAgUq3PJTm5L/etWe25xsas2vqSPttk2kURMUvSu4GTSJ1FvwcQET+R9BHgSElXRcT1pXmHA9dJ+l/S7d77kkYGPqCRISL+LemrpJqg+yT9BvgXsDzwHuDjpF/+03plS7shIhrj91wN/EPSOcA9wEhgdWBn4DDSHTvtupfU4fdrkmaT7jp7JiKu7nSuFuXPbhLw5fw5TCF19P0iqTZt3d5YTxMPADdLOou0jXuQ7tz6XkQ8lsvXl/vWrPYc2JhV2z2/qqwh6XHSeDVzgD1KzU5fIt2u+ztJ4yOi2B/i86Rg5lBgDOmLdM+IuKK4goj4paQHSDVA++S8zwL3A0eSxisZUBFxl6T1SF+yO5K260VSwDUJuKqby50jaTfSmDmnkO4Qu4YFd1D1hoPy+87ATsAdpLuS9qZvA5vTgVGkZst3ANOBb0TEqcVMfbVvzd4MVLgz0sz6iKSJpOcfrRI9GyjOzMw64T42ZmZmVhsObMzMzKw2HNiYmZlZbbiPjZmZmdWGa2zMzMysNhzYmJmZWW04sDEzM7PacGBjZmZmteHAxszMzGrj/wNPJ8AzqAWYEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 504x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize = (7,6))\n",
    "\n",
    "\n",
    "ax = sns.countplot(x='class_v2', data = annotations_df, palette=\"Set1\", linewidth=2,\n",
    "                   edgecolor='k'); #palette='mako' 'Set2'\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\")\n",
    "# ax = sns.countplot(y='label', data = df_dataset_denoised, palette=\"Set2\");\n",
    "\n",
    "\n",
    "ax.set_title('Distribution of LDED vision-audio dataset per category', fontsize = 20, pad=20);\n",
    "ax.set_xlabel(\"Experiment number\",fontsize=18, labelpad=10)\n",
    "ax.set_ylabel(\"Count\",fontsize=18, labelpad=10)\n",
    "ax.tick_params(labelsize=15)\n",
    "\n",
    "save_fig(\"dataset_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f946658-876e-493f-9d57-03dc93521464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader, device):\n",
    "    '''\n",
    "    Function for computing the accuracy of the predictions over the entire data_loader\n",
    "    '''\n",
    "    \n",
    "    correct_pred = 0 \n",
    "    n = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X, y_true in data_loader:\n",
    "\n",
    "            X = X.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "\n",
    "            _, y_prob = model(X)\n",
    "            _, predicted_labels = torch.max(y_prob, 1)\n",
    "\n",
    "            n += y_true.size(0)\n",
    "            correct_pred += (predicted_labels == y_true).sum()\n",
    "\n",
    "    return correct_pred.float() / n\n",
    "\n",
    "def plot_losses(train_losses, valid_losses):\n",
    "    '''\n",
    "    Function for plotting training and validation losses\n",
    "    '''\n",
    "    \n",
    "    # temporarily change the style of the plots to seaborn \n",
    "    plt.style.use('seaborn')\n",
    "\n",
    "    train_losses = np.array(train_losses) \n",
    "    valid_losses = np.array(valid_losses)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (8, 4.5))\n",
    "\n",
    "    ax.plot(train_losses, color='blue', label='Training loss') \n",
    "    ax.plot(valid_losses, color='red', label='Validation loss')\n",
    "    ax.set(title=\"Loss over epochs\", \n",
    "            xlabel='Epoch',\n",
    "            ylabel='Loss') \n",
    "    ax.legend()\n",
    "    fig.show()\n",
    "    \n",
    "    # change the plot style to default\n",
    "    plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31462dee-a263-4a39-b3af-f31cd9858f95",
   "metadata": {},
   "source": [
    "## Define training, testing (evaluation) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef640ed5-0922-4375-b0cf-3e0d0892923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_epoch(model, epoch, trainloader, loss_fn, optimizer, device):\n",
    "    '''\n",
    "    Function for the training single epoch in the training loop\n",
    "    '''\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    model.train() # training mode\n",
    "    running_loss = 0\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # calculate loss (forward pass)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        # backpropagate error and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # record current progress\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                    % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    print(f\"loss: {loss.item()}\")\n",
    "    epoch_loss = running_loss / len(trainloader.dataset)\n",
    "    # print(\"--------------epoch finished---------------\")\n",
    "    return model, optimizer, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71f9be35-1918-4a40-ab8d-3bfec8f34baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_epoch(model, epoch, testloader, loss_fn, device):\n",
    "    model.eval() # evaluation mode\n",
    "    global best_acc # for updating the best accuracy so far\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            ## forward pass and calculate loss\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            running_loss += loss.item() *inputs.size(0)\n",
    "            \n",
    "            # record current progress\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
    "                        % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # # Save checkpoint.\n",
    "    # acc = 100.*correct/total\n",
    "    # if acc > best_acc:\n",
    "    #     print('Saving..')\n",
    "    #     state = {\n",
    "    #         'model': model.state_dict(),\n",
    "    #         'acc': acc,\n",
    "    #         'epoch': epoch,\n",
    "    #     }\n",
    "    #     if not os.path.isdir('checkpoint'):\n",
    "    #         os.mkdir('checkpoint')\n",
    "    #     torch.save(state, './checkpoint/ckpt.pth')\n",
    "    #     best_acc = acc\n",
    "    \n",
    "    epoch_loss = test_loss / len(testloader.dataset)\n",
    "    return model, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a69f8958-82a0-4e01-9dcc-d122e628b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, loss_fn, optimizer, train_loader, valid_loader, epochs, scheduler, device, print_every=1):\n",
    "    '''\n",
    "    Function defining the entire training loop\n",
    "    '''\n",
    "    \n",
    "    # set objects for storing metrics\n",
    "    best_loss = 1e10\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    " \n",
    "    # Train model\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "        # training\n",
    "        model, optimizer, train_loss = train_single_epoch(model, epoch, train_loader, loss_fn, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # validation\n",
    "        # with torch.no_grad():\n",
    "        model, valid_loss = test_single_epoch(model, epoch, valid_loader, loss_fn, device)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "#         if epoch % print_every == (print_every - 1):\n",
    "            \n",
    "#             train_acc = get_accuracy(model, train_loader, device=device)\n",
    "#             valid_acc = get_accuracy(model, valid_loader, device=device)\n",
    "                \n",
    "#             print(f'{datetime.now().time().replace(microsecond=0)} --- '\n",
    "#                   f'Epoch: {epoch}\\t'\n",
    "#                   f'Train loss: {train_loss:.4f}\\t'\n",
    "#                   f'Valid loss: {valid_loss:.4f}\\t'\n",
    "#                   f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
    "#                   f'Valid accuracy: {100 * valid_acc:.2f}')\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    plot_losses(train_losses, valid_losses)\n",
    "    \n",
    "    return model, optimizer, (train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eecc2d-2359-46c1-b436-4aa01ca573d3",
   "metadata": {},
   "source": [
    "## Preparing Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e3fba84-b019-4257-91f7-d2b5c9e8d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------ transformation------\n",
    "train_transforms=transforms.Compose([\n",
    "    torchvision.transforms.Resize((32,32)), # original image size: (640,480)\n",
    "    transforms.RandomHorizontalFlip(),  # data augmentation\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=90),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[136.20371045013258], std=[61.9731240029325]),\n",
    "])\n",
    "\n",
    "val_transforms=transforms.Compose([\n",
    "    torchvision.transforms.Resize((32,32)), # original image size: (640,480)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[136.20371045013258], std=[61.9731240029325]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d92dcd22-0efa-4dfc-b923-8b3dbbd9fb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the total dataset:5615\n"
     ]
    }
   ],
   "source": [
    "#------ data loader------\n",
    "# create the dataset for all samples\n",
    "visiondataset = LDEDVisionDataset(annotations_df,\n",
    "                                  VISON_DIR,\n",
    "                                  train_transforms,\n",
    "                                  device)\n",
    "print (\"length of the total dataset:\" + str(len(visiondataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e625fa70-48b6-42b4-897e-932e1a93337d",
   "metadata": {},
   "source": [
    "### Dealing with Imbalanced dataset: stratified sampling\n",
    "\n",
    "- Split the data into train, validation, and test set\n",
    "- Train and Val are used for hyperparameter tuning\n",
    "- Train set will go through some data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a03b9337-9d42-4612-8f96-16953fae6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation datasets\n",
    "# train_annotations, val_annotations = train_test_split(annotations_df, test_size=0.2)\n",
    "# create the StratifiedShuffleSplit object\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "\n",
    "# split the indices of annotations into train and test sets\n",
    "train_indices, test_indices = next(sss.split(annotations_df, labels))\n",
    "\n",
    "# split the data into train and test sets\n",
    "train_annotations = annotations_df.iloc[train_indices, :]\n",
    "test_annotations = annotations_df.iloc[test_indices, :]\n",
    "\n",
    "# Create the train and validation datasets\n",
    "train_dataset = LDEDVisionDataset(train_annotations,\n",
    "                                  image_path = VISON_DIR,\n",
    "                                  image_transformation=train_transforms,\n",
    "                                  device=device)\n",
    "\n",
    "val_dataset = LDEDVisionDataset(test_annotations,\n",
    "                                image_path=VISON_DIR,\n",
    "                                image_transformation=val_transforms,\n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9faa2bc-96d4-45b8-adf5-c3e12800cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "\n",
    "# # split the indices of annotations into train, validation and test sets\n",
    "# train_indices, test_val_indices = next(sss.split(annotations_df, labels))\n",
    "\n",
    "# # Use another StratifiedShuffleSplit to split the test_val_indices into test and validation sets\n",
    "# val_size = 0.5 # set the validation size\n",
    "# sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=0)\n",
    "# val_indices, test_indices = next(sss2.split(annotations_df.iloc[test_val_indices], labels[test_val_indices]))\n",
    "\n",
    "# # split the data into train, validation, and test sets\n",
    "# train_annotations = annotations_df.iloc[train_indices, :]\n",
    "# val_annotations = annotations_df.iloc[val_indices, :]\n",
    "# test_annotations = annotations_df.iloc[test_indices, :]\n",
    "\n",
    "# # Create the train, validation and test datasets\n",
    "# train_dataset = LDEDVisionDataset(train_annotations,\n",
    "#                                   image_path = VISON_DIR,\n",
    "#                                   image_transformation=train_transforms,\n",
    "#                                   device=device)\n",
    "\n",
    "# val_dataset = LDEDVisionDataset(val_annotations,\n",
    "#                                 image_path=VISON_DIR,\n",
    "#                                 image_transformation=val_transforms,\n",
    "#                                 device=device)\n",
    "\n",
    "# test_dataset = LDEDVisionDataset(test_annotations,\n",
    "#                                  image_path=VISON_DIR,\n",
    "#                                  image_transformation=val_transforms,\n",
    "#                                  device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d7705e-1515-4116-a9e2-6d2cf40807a0",
   "metadata": {},
   "source": [
    "### Create DataLoader using the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ba1538b-bb47-4177-aec8-3c2cbae2c74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the train dataset:4492\n",
      "length of the val dataset:1123\n"
     ]
    }
   ],
   "source": [
    "# # Create train and val dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"length of the train dataset:\" +  str(len(train_dataloader.dataset)))\n",
    "print(\"length of the val dataset:\" +  str(len(val_dataloader.dataset)))\n",
    "# print(\"length of the test dataset:\" +  str(len(test_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b27ee-d1f9-48da-8e22-d322f3d13f8e",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd3050c0-b69e-4aa0-8524-86e5350465bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False\n",
    "start_epoch = 0\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e6c6580-9182-4b69-8f2b-a7e0587f45ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n",
      "--- Using CUDA ---\n",
      "\n",
      "Epoch: 0\n",
      " [================================================================>]  Step: 445ms | Tot: 45s70ms | Loss: 1.488 | Train Acc: 55.922% (2512/449 281/281  \n",
      "loss: 0.6137823462486267\n",
      " [================================================================>]  Step: 341ms | Tot: 9s837ms | Loss: 0.679 | Test Acc: 57.970% (651/112 71/71 \n",
      "\n",
      "Epoch: 1\n",
      " [================================================================>]  Step: 62ms | Tot: 24s246ms | Loss: 0.685 | Train Acc: 57.747% (2594/449 281/281  \n",
      "loss: 0.5987948775291443\n",
      " [================================================================>]  Step: 40ms | Tot: 4s469ms | Loss: 0.680 | Test Acc: 57.970% (651/112 71/71 \n",
      "\n",
      "Epoch: 2\n",
      " [================================================================>]  Step: 73ms | Tot: 23s789ms | Loss: 0.686 | Train Acc: 56.367% (2532/449 281/281  1 \n",
      "loss: 0.7392075061798096\n",
      " [================================================================>]  Step: 31ms | Tot: 4s450ms | Loss: 0.679 | Test Acc: 57.970% (651/112 71/71 \n",
      "\n",
      "Epoch: 3\n",
      " [============>....................................................]  Step: 91ms | Tot: 5s504ms | Loss: 0.684 | Train Acc: 58.553% (534/91 57/281  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----Model---------------\n",
    "print('==> Building model..')\n",
    "net = VGG('VGG19')\n",
    "# net = LeNet() \n",
    "# net = ResNet18()\n",
    "# net = PreActResNet18()\n",
    "# net = GoogLeNet()\n",
    "# net = DenseNet121()\n",
    "# net = ResNeXt29_2x64d()\n",
    "# net = MobileNet()\n",
    "# net = MobileNetV2()\n",
    "# net = DPN92()\n",
    "# net = ShuffleNetG2()\n",
    "# net = SENet18()\n",
    "# net = ShuffleNetV2(1)\n",
    "# net = EfficientNetB0()\n",
    "# net = RegNetX_200MF()\n",
    "# net = SimpleDLA()\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "    print (\"--- Using CUDA ---\")\n",
    "\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Note: weight_decay in pytorch is L2 regularization\n",
    "# optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE,\n",
    "#                     momentum=0.9, weight_decay=5e-4)\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+EPOCHS):\n",
    "    train_single_epoch(net, epoch, train_dataloader, loss_fn, optimizer, device)\n",
    "    test_single_epoch(net, epoch, val_dataloader, loss_fn, device)\n",
    "    scheduler.step()\n",
    "\n",
    "# model, optimizer, _ = training_loop(net, loss_fn, optimizer, train_dataloader, val_dataloader, EPOCHS, scheduler, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
