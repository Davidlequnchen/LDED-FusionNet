{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd983b1d-7748-47e3-95bd-f9ebb2c7dbab",
   "metadata": {},
   "source": [
    "# Multisensor fusion project\n",
    "- Experiment data: June 2022. Maraging Steel 300\n",
    "- Experiment number (single bead wall samples): 21-26\n",
    "- Recorded data: position, veolocity, coaxial ccd features, acoustic feature, thermal features\n",
    "- Defect generated: keyhole pores, cracks, defect-free\n",
    "\n",
    "### Notebook 1: Vision acoustic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c15e44-7af9-4991-a519-58ce15c05229",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Required python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "#---------------opencv------------------------\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8411ced-724d-429c-a5a6-925df20c63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scaleogram as scg \n",
    "from glob import glob\n",
    "import scipy\n",
    "from scipy.signal import welch\n",
    "import wave                    # library handles the parsing of WAV file headers\n",
    "import pywt\n",
    "import soundfile as sf\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00f2471-373d-4feb-a277-d1d1ef7fc8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- plotly visualizatoin----------------------------------\n",
    "from PIL import Image\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from skimage import data\n",
    "\n",
    "from glob import glob\n",
    "import glob\n",
    "import scipy\n",
    "from scipy.signal import welch\n",
    "import wave                    # library handles the parsing of WAV file headers\n",
    "import pywt\n",
    "import soundfile as sf\n",
    "import matplotlib.font_manager as font_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5833c33f-e476-4cc6-a0a7-d40dfb25fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Audio signal processing libraries\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "# import nussl\n",
    "import natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee12e1ef-236d-4a31-9a0b-2addfe3913a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures, and dataset locations\n",
    "PROJECT_ROOT_DIR = \"../\"\n",
    "\n",
    "Audio_ROOT = os.path.join(\"C:\\\\Users\\\\Asus\\\\OneDrive_Chen1470\\\\OneDrive - Nanyang Technological University\\\\Dataset\\\\LDED_audio_dataset\")\n",
    "             \n",
    "Audio_PATH_original = os.path.join(Audio_ROOT, 'wave_file', \"original\")\n",
    "Audio_PATH_equalized = os.path.join(Audio_ROOT, 'wave_file', \"equalized\")\n",
    "Audio_PATH_bandpassed = os.path.join(Audio_ROOT, 'wave_file', \"bandpassed\")\n",
    "Audio_PATH_denoised = os.path.join(Audio_ROOT, 'wave_file', \"denoised\",)\n",
    "\n",
    "\n",
    "Multimodal_dataset_PATH = os.path.join(\"C:\\\\Users\\\\Asus\\\\OneDrive_Chen1470\\\\OneDrive - Nanyang Technological University\\\\Dataset\\\\Multimodal_AM_monitoring\\\\LDED_Acoustic_Visual_Dataset\")\n",
    "Audio_segmented_30Hz_PATH_24 = os.path.join(Multimodal_dataset_PATH, 'Audio_signal_all_30Hz_24')\n",
    "Audio_segmented_30Hz_PATH = os.path.join(Multimodal_dataset_PATH, 'Audio_signal_all_30Hz')\n",
    "CCD_Image_30Hz_path_22 = os.path.join(Multimodal_dataset_PATH, 'Coaxial_CCD_images_30Hz')\n",
    "CCD_Image_30Hz_path_24 = os.path.join(Multimodal_dataset_PATH, 'coaxial_meltpool_images_24_30Hz')\n",
    "\n",
    "Video_path = os.path.join(Multimodal_dataset_PATH, 'Video')\n",
    "\n",
    "IMAGE_PATH = os.path.join(PROJECT_ROOT_DIR, \"result_images\", 'pre-processing')\n",
    "\n",
    "os.makedirs(IMAGE_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "## function for automatically save the diagram/graph into the folder \n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGE_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "061cb9f9-bdf5-4014-a60c-7172cb660f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_visualization(sound, sr=44100, alpha = 1):\n",
    "    fig, axs = plt.subplots(1, 1, tight_layout = True, figsize=(6, 4)) #constrained_layout=True,\n",
    "\n",
    "    librosa.display.waveplot(sound, sr=sr, alpha=alpha, label = 'original signal')\n",
    "    axs.set_xlabel('Time [sec]', fontsize = 14)\n",
    "    axs.set_ylabel('Amplitute', fontsize = 14)\n",
    "    axs.set_ylim([-0.28, 0.28])\n",
    "    axs.set_yticks([-0.25, 0, 0.25])\n",
    "\n",
    "    # fig.suptitle(\"Time-domain visualisation\", fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56593565-9c2b-4a72-a553-5ae9c8d56bab",
   "metadata": {},
   "source": [
    "## Segment MP4 video file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224503e2-02f3-436e-9248-06029387682f",
   "metadata": {},
   "source": [
    "- uses __FFmpeg__ to extract the audio stream from the input MP4 file\n",
    "- uses __OpenCV__ to extract the video frames\n",
    "- __iterates over each frame__ and uses FFmpeg to extract the corresponding audio segment; OpenCV to extract the corresponding video segment\n",
    "- The audio and image segments are then saved to their respective output folders with the desired names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41db77-c0ce-4722-b458-4a84afe1d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Define input video file path and output folders\n",
    "input_file = os.path.join(Video_path, '22_denoised_video.mp4') \n",
    "image_output_folder = os.path.join(Video_path, 'image_segments')\n",
    "audio_output_folder = os.path.join(Video_path, 'audio_segments')\n",
    "\n",
    "# Create output folders if they don't exist\n",
    "if not os.path.exists(image_output_folder):\n",
    "    os.makedirs(image_output_folder)\n",
    "if not os.path.exists(audio_output_folder):\n",
    "    os.makedirs(audio_output_folder)\n",
    "\n",
    "# Define segment duration in seconds\n",
    "segment_duration = 1/10  # 10 frames per second\n",
    "\n",
    "# Use FFmpeg to extract the audio and video streams\n",
    "subprocess.call(['ffmpeg', '-i', input_file, '-vn', '-acodec', 'copy', f'{audio_output_folder}/%d.wav'])\n",
    "\n",
    "# Use OpenCV to extract the video frames\n",
    "import cv2\n",
    "cap = cv2.VideoCapture(input_file)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Iterate over the video frames and save each segment\n",
    "for i in range(frame_count):\n",
    "    # Calculate start and end time of current segment\n",
    "    start_time = i / fps\n",
    "    end_time = (i + 1) / fps\n",
    "\n",
    "    # Use FFmpeg to extract the audio segment\n",
    "    subprocess.call(['ffmpeg', '-i', input_file, '-vn', '-ss', f'{start_time}', '-t', f'{segment_duration}', '-acodec', 'copy', f'{audio_output_folder}/{i+1}.wav'])\n",
    "\n",
    "    # Use OpenCV to extract the video segment\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, start_time*1000)\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imwrite(f'{image_output_folder}/{i+1}.jpg', frame)\n",
    "\n",
    "# Release resources\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
